{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c4bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff63f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pooja\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the values for the new data point separated by commas: -76,-83,-70,-66,-64,-72,-64,-69,-60,-76,-83,-78,-81,-81,-81,-70,-60,-60\n",
      "The data point belongs to cluster 1 because it is closest to the cluster center at a distance of 3.99.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pooja\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"C:\\Users\\pooja\\Downloads\\test.csv\"  # Update with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Standardize the data (excluding the target column if there is one)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Step 2: Clustering\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(scaled_data)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Step 3: Assign Clusters to New Data Points\n",
    "def assign_cluster(new_data_point):\n",
    "    # Standardize the new data point using the same scaler\n",
    "    scaled_new_data = scaler.transform([new_data_point])\n",
    "    \n",
    "    # Predict the cluster for the new data point\n",
    "    cluster_label = kmeans.predict(scaled_new_data)[0]\n",
    "    \n",
    "    # Find the cluster center\n",
    "    cluster_center = kmeans.cluster_centers_[cluster_label]\n",
    "    \n",
    "    # Calculate the distance to the cluster center\n",
    "    distance = np.linalg.norm(scaled_new_data - cluster_center)\n",
    "    \n",
    "    # Explain the assignment\n",
    "    explanation = f\"The data point belongs to cluster {cluster_label} because it is closest to the cluster center at a distance of {distance:.2f}.\"\n",
    "    \n",
    "    return cluster_label, explanation\n",
    "\n",
    "# User input for new data point\n",
    "new_data_point = input(\"Enter the values for the new data point separated by commas: \").split(',')\n",
    "new_data_point = [float(val.strip()) for val in new_data_point]\n",
    "\n",
    "# Assign cluster and provide explanation\n",
    "cluster_label, explanation = assign_cluster(new_data_point)\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5092c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_classifier_model.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load train and test data from CSV files\n",
    "train_data = pd.read_csv(r'C:\\Users\\pooja\\Downloads\\train.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\pooja\\Downloads\\test.csv')\n",
    "\n",
    "# Separate features and target variable for train data\n",
    "X_train = train_data.drop(columns=['target'])\n",
    "y_train = train_data['target']\n",
    "\n",
    "# Separate features for test data\n",
    "X_test = test_data  # Assuming test data contains only features\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict target values for the test data\n",
    "y_pred_test = rf_classifier.predict(X_test)\n",
    "\n",
    "# Convert predicted target values to DataFrame\n",
    "predictions_test_df = pd.DataFrame({'predicted_target': y_pred_test})\n",
    "\n",
    "# Save predicted target values to a CSV file\n",
    "predictions_test_df.to_csv('predicted_target_test.csv', index=False)\n",
    "\n",
    "# Save the trained model for future use\n",
    "joblib.dump(rf_classifier, 'rf_classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f6c5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Load train and test data from CSV files\n",
    "train_data = pd.read_csv(r'C:\\Users\\pooja\\Downloads\\train.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\pooja\\Downloads\\test.csv')\n",
    "\n",
    "# Separate features and target variable for train data\n",
    "X_train = train_data.drop(columns=['target'])\n",
    "y_train = train_data['target']\n",
    "\n",
    "# Separate features for test data\n",
    "X_test = test_data  # Assuming test data contains only features\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict target values for the test data\n",
    "y_pred_test = rf_classifier.predict(X_test)\n",
    "\n",
    "# Convert predicted target values to DataFrame\n",
    "predictions_test_df = pd.DataFrame({'predicted_target': y_pred_test})\n",
    "\n",
    "# Save predicted target values to a CSV file\n",
    "predictions_test_df.to_csv('predicted_target_test.csv', index=False)\n",
    "\n",
    "# Save the trained model using pickle in a temporary directory\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    model_file_path = os.path.join(temp_dir, 'rf_classifier_model.pkl')\n",
    "    with open(model_file_path, 'wb') as file:\n",
    "        pickle.dump(rf_classifier, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fcb2b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: Index(['date', 'time', 'sensor', 'location', 'number', 'activity', 'position',\n",
      "       'location.1'],\n",
      "      dtype='object')\n",
      "Summary saved to C:\\Users\\pooja\\Downloads\\activity_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the raw data from the CSV file\n",
    "raw_data = pd.read_csv(r'C:\\Users\\pooja\\Downloads\\rawdata.csv')\n",
    "\n",
    "# Print the column names to inspect them\n",
    "print(\"Column names:\", raw_data.columns)\n",
    "\n",
    "# Combine 'date' and 'time' columns to create a 'timestamp' column\n",
    "raw_data['timestamp'] = pd.to_datetime(raw_data['date'] + ' ' + raw_data['time'])\n",
    "\n",
    "# Extract the date from the 'timestamp' column\n",
    "raw_data['date'] = raw_data['timestamp'].dt.date\n",
    "\n",
    "# Count the number of 'picking' and 'placing' activities per date\n",
    "activity_count = raw_data[raw_data['activity'].isin(['picking', 'placing'])].groupby(['date', 'activity']).size().unstack(fill_value=0)\n",
    "\n",
    "# Count the number of 'inside' and 'outside' activities per date\n",
    "location_count = raw_data.groupby(['date', 'location']).size().unstack(fill_value=0)\n",
    "\n",
    "# Merge the activity and location count dataframes\n",
    "summary = pd.concat([location_count, activity_count], axis=1).reset_index()\n",
    "\n",
    "# Save the result to a CSV file\n",
    "summary.to_csv(r'C:\\Users\\pooja\\Downloads\\activity_summary.csv', index=False)\n",
    "\n",
    "print(\"Summary saved to C:\\\\Users\\\\pooja\\\\Downloads\\\\activity_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763c06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
